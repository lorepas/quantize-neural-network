[32m[0716 13:39:00 @logger.py:92][0m Argv: /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-58044733-aaf5-4691-8bda-2d09249537d3.json
[32m[0716 13:39:00 @fs.py:101][0m [5m[31mWRN[0m Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.
[32m[0716 13:39:00 @fs.py:104][0m Created the directory /root/tensorpack_data.
[32m[0716 13:39:00 @svhn.py:42][0m File /root/tensorpack_data/svhn_data/train_32x32.mat not found!
[32m[0716 13:39:00 @svhn.py:43][0m Downloading from http://ufldl.stanford.edu/housenumbers/train_32x32.mat ...
[32m[0716 13:39:03 @fs.py:73][0m Succesfully downloaded train_32x32.mat. 182040794 bytes.
[32m[0716 13:39:03 @svhn.py:45][0m Loading /root/tensorpack_data/svhn_data/train_32x32.mat ...
[32m[0716 13:39:05 @svhn.py:42][0m File /root/tensorpack_data/svhn_data/extra_32x32.mat not found!
[32m[0716 13:39:05 @svhn.py:43][0m Downloading from http://ufldl.stanford.edu/housenumbers/extra_32x32.mat ...
[32m[0716 13:40:10 @fs.py:73][0m Succesfully downloaded extra_32x32.mat. 1329278602 bytes.
[32m[0716 13:40:10 @svhn.py:45][0m Loading /root/tensorpack_data/svhn_data/extra_32x32.mat ...
[32m[0716 13:40:22 @svhn.py:42][0m File /root/tensorpack_data/svhn_data/test_32x32.mat not found!
[32m[0716 13:40:22 @svhn.py:43][0m Downloading from http://ufldl.stanford.edu/housenumbers/test_32x32.mat ...
[32m[0716 13:40:25 @fs.py:73][0m Succesfully downloaded test_32x32.mat. 64275384 bytes.
[32m[0716 13:40:25 @svhn.py:45][0m Loading /root/tensorpack_data/svhn_data/test_32x32.mat ...
[32m[0716 13:40:25 @parallel.py:340][0m [MultiProcessRunnerZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0716 13:40:25 @input_source.py:221][0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[32m[0716 13:40:25 @trainers.py:48][0m Building graph for a single training tower ...
[32m[0716 13:40:25 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv0/W
[32m[0716 13:40:25 @registry.py:90][0m 'conv0': [?, 40, 40, 3] --> [?, 36, 36, 48]
[32m[0716 13:40:25 @registry.py:90][0m 'pool0': [?, 36, 36, 48] --> [?, 18, 18, 48]
[32m[0716 13:40:25 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv1/W
[32m[0716 13:40:25 @registry.py:90][0m 'conv1': [?, 18, 18, 48] --> [?, 18, 18, 64]
[32m[0716 13:40:26 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv2/W
[32m[0716 13:40:26 @registry.py:90][0m 'conv2': [?, 18, 18, 64] --> [?, 18, 18, 64]
[32m[0716 13:40:26 @registry.py:90][0m 'pool1': [?, 18, 18, 64] --> [?, 9, 9, 64]
[32m[0716 13:40:26 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv3/W
[32m[0716 13:40:26 @registry.py:90][0m 'conv3': [?, 9, 9, 64] --> [?, 7, 7, 128]
[32m[0716 13:40:26 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv4/W
[32m[0716 13:40:26 @registry.py:90][0m 'conv4': [?, 7, 7, 128] --> [?, 7, 7, 128]
[32m[0716 13:40:26 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv5/W
[32m[0716 13:40:26 @registry.py:90][0m 'conv5': [?, 7, 7, 128] --> [?, 5, 5, 128]
[32m[0716 13:40:26 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv6/W
[32m[0716 13:40:26 @registry.py:90][0m 'conv6': [?, 5, 5, 128] --> [?, 1, 1, 512]
[32m[0716 13:40:26 @<ipython-input-3-92482375c899>:113][0m Binarizing weight fc1/W
[32m[0716 13:40:26 @registry.py:90][0m 'fc1': [?, 1, 1, 512] --> [?, 10]
[32m[0716 13:40:26 @regularize.py:97][0m regularize_cost() found 1 variables to regularize.
[32m[0716 13:40:26 @regularize.py:21][0m The following tensors will be regularized: fc1/W:0
[32m[0716 13:40:27 @model_utils.py:67][0m [36mList of Trainable Variables: 
[0mname       shape               #elements
---------  ----------------  -----------
conv0/W    [5, 5, 3, 48]            3600
conv0/b    [48]                       48
conv1/W    [3, 3, 48, 64]          27648
bn1/gamma  [64]                       64
bn1/beta   [64]                       64
conv2/W    [3, 3, 64, 64]          36864
bn2/gamma  [64]                       64
bn2/beta   [64]                       64
conv3/W    [3, 3, 64, 128]         73728
bn3/gamma  [128]                     128
bn3/beta   [128]                     128
conv4/W    [3, 3, 128, 128]       147456
bn4/gamma  [128]                     128
bn4/beta   [128]                     128
conv5/W    [3, 3, 128, 128]       147456
bn5/gamma  [128]                     128
bn5/beta   [128]                     128
conv6/W    [5, 5, 128, 512]      1638400
bn6/gamma  [512]                     512
bn6/beta   [512]                     512
fc1/W      [512, 10]                5120
fc1/b      [10]                       10[36m
Number of trainable variables: 22
Number of parameters (elements): 2082378
Storage space needed for all trainable variables: 7.94MB[0m
[32m[0716 13:40:27 @base.py:207][0m Setup callbacks graph ...
[32m[0716 13:40:27 @argtools.py:138][0m [5m[31mWRN[0m Starting a process with 'fork' method is efficient but not safe and may cause deadlock or crash.Use 'forkserver' or 'spawn' method instead if you run into such issues.See https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods on how to set them.
[32m[0716 13:40:27 @argtools.py:138][0m [5m[31mWRN[0m "import prctl" failed! Install python-prctl so that processes can be cleaned with guarantee.
[32m[0716 13:40:27 @inference_runner.py:148][0m [InferenceRunner] Building tower 'InferenceTower' on device /gpu:0 ...
[32m[0716 13:40:27 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv0/W
[32m[0716 13:40:27 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv1/W
[32m[0716 13:40:28 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv2/W
[32m[0716 13:40:28 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv3/W
[32m[0716 13:40:28 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv4/W
[32m[0716 13:40:28 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv5/W
[32m[0716 13:40:28 @<ipython-input-3-92482375c899>:113][0m Binarizing weight conv6/W
[32m[0716 13:40:28 @<ipython-input-3-92482375c899>:113][0m Binarizing weight fc1/W
[32m[0716 13:40:29 @summary.py:47][0m [MovingAverageSummary] 5 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[32m[0716 13:40:29 @summary.py:94][0m Summarizing collection 'summaries' of size 22.
[32m[0716 13:40:29 @graph.py:99][0m Applying collection UPDATE_OPS of 12 ops.
[32m[0716 13:40:29 @base.py:228][0m Creating the session ...
[32m[0716 13:40:36 @base.py:234][0m Initializing the session ...
[32m[0716 13:40:36 @base.py:241][0m Graph Finalized.
[32m[0716 13:40:36 @concurrency.py:37][0m Starting EnqueueThread: enqueue dataflow to TF queue "QueueInput/input_queue" ...
[32m[0716 13:40:37 @inference_runner.py:95][0m [InferenceRunner] Will eval 204 iterations
[32m[0716 13:40:37 @monitor.py:361][0m [5m[31mWRN[0m History epoch=10 from JSON is not the predecessor of the current starting_epoch=1
[32m[0716 13:40:37 @monitor.py:362][0m [5m[31mWRN[0m If you want to resume old training, either use `AutoResumeTrainConfig` or correctly set the new starting_epoch yourself to avoid inconsistency. 
[32m[0716 13:40:37 @monitor.py:369][0m [5m[31mWRN[0m Now, we will train with starting_epoch=1 and backup old json to train_log/svhn-dorefa-1,2,4/stats.json.0716-134037
[32m[0716 13:40:37 @base.py:273][0m Start Epoch 1 ...
[32m[0716 13:45:08 @base.py:283][0m Epoch 1 (global_step 4721) finished, time:4 minutes 30 seconds.
[32m[0716 13:45:08 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-4721.
[32m[0716 13:45:19 @monitor.py:476][0m QueueInput/queue_size: 17.5
[32m[0716 13:45:19 @monitor.py:476][0m accuracy: 0.9514
[32m[0716 13:45:19 @monitor.py:476][0m cost: 0.17174
[32m[0716 13:45:19 @monitor.py:476][0m cross_entropy_loss: 0.17174
[32m[0716 13:45:19 @monitor.py:476][0m param-summary/conv0/W-rms: 0.18579
[32m[0716 13:45:19 @monitor.py:476][0m param-summary/conv1/W-rms: 0.079097
[32m[0716 13:45:19 @monitor.py:476][0m param-summary/conv2/W-rms: 0.073921
[32m[0716 13:45:19 @monitor.py:476][0m param-summary/conv3/W-rms: 0.078041
[32m[0716 13:45:19 @monitor.py:476][0m param-summary/conv4/W-rms: 0.065342
[32m[0716 13:45:19 @monitor.py:476][0m param-summary/conv5/W-rms: 0.069217
[32m[0716 13:45:19 @monitor.py:476][0m param-summary/conv6/W-rms: 0.054508
[32m[0716 13:45:19 @monitor.py:476][0m param-summary/fc1/W-rms: 0.11674
[32m[0716 13:45:19 @monitor.py:476][0m regularize_cost: 3.4822e-06
[32m[0716 13:45:19 @monitor.py:476][0m train_error: 0.048603
[32m[0716 13:45:19 @monitor.py:476][0m val_accuracy: 0.91836
[32m[0716 13:45:19 @monitor.py:476][0m val_cross_entropy_loss: 0.28042
[32m[0716 13:45:19 @group.py:44][0m Callbacks took 11.442 sec in total. InferenceRunner: 10.8 seconds
[32m[0716 13:45:19 @base.py:273][0m Start Epoch 2 ...
[32m[0716 13:49:22 @base.py:283][0m Epoch 2 (global_step 9442) finished, time:4 minutes 3 seconds.
[32m[0716 13:49:22 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-9442.
[32m[0716 13:49:32 @monitor.py:476][0m QueueInput/queue_size: 0.125
[32m[0716 13:49:32 @monitor.py:476][0m accuracy: 0.95954
[32m[0716 13:49:32 @monitor.py:476][0m cost: 0.12696
[32m[0716 13:49:32 @monitor.py:476][0m cross_entropy_loss: 0.12695
[32m[0716 13:49:32 @monitor.py:476][0m param-summary/conv0/W-rms: 0.21594
[32m[0716 13:49:32 @monitor.py:476][0m param-summary/conv1/W-rms: 0.095424
[32m[0716 13:49:32 @monitor.py:476][0m param-summary/conv2/W-rms: 0.096639
[32m[0716 13:49:32 @monitor.py:476][0m param-summary/conv3/W-rms: 0.10256
[32m[0716 13:49:32 @monitor.py:476][0m param-summary/conv4/W-rms: 0.090783
[32m[0716 13:49:32 @monitor.py:476][0m param-summary/conv5/W-rms: 0.095921
[32m[0716 13:49:32 @monitor.py:476][0m param-summary/conv6/W-rms: 0.080853
[32m[0716 13:49:32 @monitor.py:476][0m param-summary/fc1/W-rms: 0.15417
[32m[0716 13:49:32 @monitor.py:476][0m regularize_cost: 6.0714e-06
[32m[0716 13:49:32 @monitor.py:476][0m train_error: 0.040459
[32m[0716 13:49:32 @monitor.py:476][0m val_accuracy: 0.9346
[32m[0716 13:49:32 @monitor.py:476][0m val_cross_entropy_loss: 0.2344
[32m[0716 13:49:32 @group.py:44][0m Callbacks took 9.864 sec in total. InferenceRunner: 9.63 seconds
[32m[0716 13:49:32 @base.py:273][0m Start Epoch 3 ...
[32m[0716 13:53:37 @base.py:283][0m Epoch 3 (global_step 14163) finished, time:4 minutes 5 seconds.
[32m[0716 13:53:37 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-14163.
[32m[0716 13:53:47 @monitor.py:476][0m QueueInput/queue_size: 1.4927e-08
[32m[0716 13:53:47 @monitor.py:476][0m accuracy: 0.96718
[32m[0716 13:53:47 @monitor.py:476][0m cost: 0.11572
[32m[0716 13:53:47 @monitor.py:476][0m cross_entropy_loss: 0.11571
[32m[0716 13:53:47 @monitor.py:476][0m param-summary/conv0/W-rms: 0.24038
[32m[0716 13:53:47 @monitor.py:476][0m param-summary/conv1/W-rms: 0.11408
[32m[0716 13:53:47 @monitor.py:476][0m param-summary/conv2/W-rms: 0.11999
[32m[0716 13:53:47 @monitor.py:476][0m param-summary/conv3/W-rms: 0.1263
[32m[0716 13:53:47 @monitor.py:476][0m param-summary/conv4/W-rms: 0.11397
[32m[0716 13:53:47 @monitor.py:476][0m param-summary/conv5/W-rms: 0.1194
[32m[0716 13:53:47 @monitor.py:476][0m param-summary/conv6/W-rms: 0.1028
[32m[0716 13:53:47 @monitor.py:476][0m param-summary/fc1/W-rms: 0.18507
[32m[0716 13:53:47 @monitor.py:476][0m regularize_cost: 8.7553e-06
[32m[0716 13:53:47 @monitor.py:476][0m train_error: 0.032823
[32m[0716 13:53:47 @monitor.py:476][0m val_accuracy: 0.9481
[32m[0716 13:53:47 @monitor.py:476][0m val_cross_entropy_loss: 0.19385
[32m[0716 13:53:47 @group.py:44][0m Callbacks took 9.808 sec in total. InferenceRunner: 9.53 seconds
[32m[0716 13:53:47 @base.py:273][0m Start Epoch 4 ...
[32m[0716 13:57:53 @base.py:283][0m Epoch 4 (global_step 18884) finished, time:4 minutes 5 seconds.
[32m[0716 13:57:53 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-18884.
[32m[0716 13:58:03 @monitor.py:476][0m QueueInput/queue_size: 1.3885e-17
[32m[0716 13:58:03 @monitor.py:476][0m accuracy: 0.97246
[32m[0716 13:58:03 @monitor.py:476][0m cost: 0.10792
[32m[0716 13:58:03 @monitor.py:476][0m cross_entropy_loss: 0.10791
[32m[0716 13:58:03 @monitor.py:476][0m param-summary/conv0/W-rms: 0.25949
[32m[0716 13:58:03 @monitor.py:476][0m param-summary/conv1/W-rms: 0.13054
[32m[0716 13:58:03 @monitor.py:476][0m param-summary/conv2/W-rms: 0.14091
[32m[0716 13:58:03 @monitor.py:476][0m param-summary/conv3/W-rms: 0.14817
[32m[0716 13:58:03 @monitor.py:476][0m param-summary/conv4/W-rms: 0.13491
[32m[0716 13:58:03 @monitor.py:476][0m param-summary/conv5/W-rms: 0.1409
[32m[0716 13:58:03 @monitor.py:476][0m param-summary/conv6/W-rms: 0.12108
[32m[0716 13:58:03 @monitor.py:476][0m param-summary/fc1/W-rms: 0.21065
[32m[0716 13:58:03 @monitor.py:476][0m regularize_cost: 1.1334e-05
[32m[0716 13:58:03 @monitor.py:476][0m train_error: 0.027544
[32m[0716 13:58:03 @monitor.py:476][0m val_accuracy: 0.95005
[32m[0716 13:58:03 @monitor.py:476][0m val_cross_entropy_loss: 0.19149
[32m[0716 13:58:03 @group.py:44][0m Callbacks took 10.121 sec in total. InferenceRunner: 9.88 seconds
[32m[0716 13:58:03 @base.py:273][0m Start Epoch 5 ...
[32m[0716 14:02:10 @base.py:283][0m Epoch 5 (global_step 23605) finished, time:4 minutes 7 seconds.
[32m[0716 14:02:10 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-23605.
[32m[0716 14:02:19 @monitor.py:476][0m QueueInput/queue_size: 7.6611e-06
[32m[0716 14:02:19 @monitor.py:476][0m accuracy: 0.97374
[32m[0716 14:02:19 @monitor.py:476][0m cost: 0.093331
[32m[0716 14:02:19 @monitor.py:476][0m cross_entropy_loss: 0.093316
[32m[0716 14:02:19 @monitor.py:476][0m param-summary/conv0/W-rms: 0.2764
[32m[0716 14:02:19 @monitor.py:476][0m param-summary/conv1/W-rms: 0.14687
[32m[0716 14:02:19 @monitor.py:476][0m param-summary/conv2/W-rms: 0.16057
[32m[0716 14:02:19 @monitor.py:476][0m param-summary/conv3/W-rms: 0.16751
[32m[0716 14:02:19 @monitor.py:476][0m param-summary/conv4/W-rms: 0.1537
[32m[0716 14:02:19 @monitor.py:476][0m param-summary/conv5/W-rms: 0.1608
[32m[0716 14:02:19 @monitor.py:476][0m param-summary/conv6/W-rms: 0.13727
[32m[0716 14:02:19 @monitor.py:476][0m param-summary/fc1/W-rms: 0.23683
[32m[0716 14:02:19 @monitor.py:476][0m regularize_cost: 1.4342e-05
[32m[0716 14:02:19 @monitor.py:476][0m train_error: 0.026265
[32m[0716 14:02:19 @monitor.py:476][0m val_accuracy: 0.9559
[32m[0716 14:02:19 @monitor.py:476][0m val_cross_entropy_loss: 0.16807
[32m[0716 14:02:19 @group.py:44][0m Callbacks took 9.563 sec in total. InferenceRunner: 9.32 seconds
[32m[0716 14:02:19 @base.py:273][0m Start Epoch 6 ...
[32m[0716 14:06:28 @base.py:283][0m Epoch 6 (global_step 28326) finished, time:4 minutes 8 seconds.
[32m[0716 14:06:28 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-28326.
[32m[0716 14:06:38 @monitor.py:476][0m QueueInput/queue_size: 0.0019531
[32m[0716 14:06:38 @monitor.py:476][0m accuracy: 0.97299
[32m[0716 14:06:38 @monitor.py:476][0m cost: 0.087401
[32m[0716 14:06:38 @monitor.py:476][0m cross_entropy_loss: 0.087383
[32m[0716 14:06:38 @monitor.py:476][0m param-summary/conv0/W-rms: 0.28857
[32m[0716 14:06:38 @monitor.py:476][0m param-summary/conv1/W-rms: 0.16188
[32m[0716 14:06:38 @monitor.py:476][0m param-summary/conv2/W-rms: 0.17928
[32m[0716 14:06:38 @monitor.py:476][0m param-summary/conv3/W-rms: 0.18623
[32m[0716 14:06:38 @monitor.py:476][0m param-summary/conv4/W-rms: 0.17088
[32m[0716 14:06:38 @monitor.py:476][0m param-summary/conv5/W-rms: 0.1789
[32m[0716 14:06:38 @monitor.py:476][0m param-summary/conv6/W-rms: 0.15179
[32m[0716 14:06:38 @monitor.py:476][0m param-summary/fc1/W-rms: 0.26283
[32m[0716 14:06:38 @monitor.py:476][0m regularize_cost: 1.7671e-05
[32m[0716 14:06:38 @monitor.py:476][0m train_error: 0.02701
[32m[0716 14:06:38 @monitor.py:476][0m val_accuracy: 0.9574
[32m[0716 14:06:38 @monitor.py:476][0m val_cross_entropy_loss: 0.16052
[32m[0716 14:06:38 @group.py:44][0m Callbacks took 9.640 sec in total. InferenceRunner: 9.38 seconds
[32m[0716 14:06:38 @base.py:273][0m Start Epoch 7 ...
[32m[0716 14:10:41 @base.py:283][0m Epoch 7 (global_step 33047) finished, time:4 minutes 3 seconds.
[32m[0716 14:10:41 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-33047.
[32m[0716 14:10:51 @monitor.py:476][0m QueueInput/queue_size: 0.064484
[32m[0716 14:10:51 @monitor.py:476][0m accuracy: 0.97438
[32m[0716 14:10:51 @monitor.py:476][0m cost: 0.090261
[32m[0716 14:10:51 @monitor.py:476][0m cross_entropy_loss: 0.09024
[32m[0716 14:10:51 @monitor.py:476][0m param-summary/conv0/W-rms: 0.3014
[32m[0716 14:10:51 @monitor.py:476][0m param-summary/conv1/W-rms: 0.17805
[32m[0716 14:10:51 @monitor.py:476][0m param-summary/conv2/W-rms: 0.19681
[32m[0716 14:10:51 @monitor.py:476][0m param-summary/conv3/W-rms: 0.20279
[32m[0716 14:10:51 @monitor.py:476][0m param-summary/conv4/W-rms: 0.18721
[32m[0716 14:10:51 @monitor.py:476][0m param-summary/conv5/W-rms: 0.19505
[32m[0716 14:10:51 @monitor.py:476][0m param-summary/conv6/W-rms: 0.16499
[32m[0716 14:10:51 @monitor.py:476][0m param-summary/fc1/W-rms: 0.28735
[32m[0716 14:10:51 @monitor.py:476][0m regularize_cost: 2.1124e-05
[32m[0716 14:10:51 @monitor.py:476][0m train_error: 0.02562
[32m[0716 14:10:51 @monitor.py:476][0m val_accuracy: 0.95861
[32m[0716 14:10:51 @monitor.py:476][0m val_cross_entropy_loss: 0.15994
[32m[0716 14:10:51 @group.py:44][0m Callbacks took 9.638 sec in total. InferenceRunner: 9.4 seconds
[32m[0716 14:10:51 @base.py:273][0m Start Epoch 8 ...
[32m[0716 14:14:55 @base.py:283][0m Epoch 8 (global_step 37768) finished, time:4 minutes 4 seconds.
[32m[0716 14:14:55 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-37768.
[32m[0716 14:15:05 @monitor.py:476][0m QueueInput/queue_size: 0.015672
[32m[0716 14:15:05 @monitor.py:476][0m accuracy: 0.97368
[32m[0716 14:15:05 @monitor.py:476][0m cost: 0.096085
[32m[0716 14:15:05 @monitor.py:476][0m cross_entropy_loss: 0.09606
[32m[0716 14:15:05 @monitor.py:476][0m param-summary/conv0/W-rms: 0.31298
[32m[0716 14:15:05 @monitor.py:476][0m param-summary/conv1/W-rms: 0.19409
[32m[0716 14:15:05 @monitor.py:476][0m param-summary/conv2/W-rms: 0.21388
[32m[0716 14:15:05 @monitor.py:476][0m param-summary/conv3/W-rms: 0.21813
[32m[0716 14:15:05 @monitor.py:476][0m param-summary/conv4/W-rms: 0.20263
[32m[0716 14:15:05 @monitor.py:476][0m param-summary/conv5/W-rms: 0.21149
[32m[0716 14:15:05 @monitor.py:476][0m param-summary/conv6/W-rms: 0.17743
[32m[0716 14:15:05 @monitor.py:476][0m param-summary/fc1/W-rms: 0.31147
[32m[0716 14:15:05 @monitor.py:476][0m regularize_cost: 2.4829e-05
[32m[0716 14:15:05 @monitor.py:476][0m train_error: 0.026321
[32m[0716 14:15:05 @monitor.py:476][0m val_accuracy: 0.95447
[32m[0716 14:15:05 @monitor.py:476][0m val_cross_entropy_loss: 0.17118
[32m[0716 14:15:05 @group.py:44][0m Callbacks took 10.043 sec in total. InferenceRunner: 9.79 seconds
[32m[0716 14:15:05 @base.py:273][0m Start Epoch 9 ...
[32m[0716 14:19:08 @base.py:283][0m Epoch 9 (global_step 42489) finished, time:4 minutes 3 seconds.
[32m[0716 14:19:09 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-42489.
[32m[0716 14:19:18 @monitor.py:476][0m QueueInput/queue_size: 0.25
[32m[0716 14:19:18 @monitor.py:476][0m accuracy: 0.97917
[32m[0716 14:19:18 @monitor.py:476][0m cost: 0.069874
[32m[0716 14:19:18 @monitor.py:476][0m cross_entropy_loss: 0.069845
[32m[0716 14:19:18 @monitor.py:476][0m param-summary/conv0/W-rms: 0.32198
[32m[0716 14:19:18 @monitor.py:476][0m param-summary/conv1/W-rms: 0.20752
[32m[0716 14:19:18 @monitor.py:476][0m param-summary/conv2/W-rms: 0.2301
[32m[0716 14:19:18 @monitor.py:476][0m param-summary/conv3/W-rms: 0.23279
[32m[0716 14:19:18 @monitor.py:476][0m param-summary/conv4/W-rms: 0.21691
[32m[0716 14:19:18 @monitor.py:476][0m param-summary/conv5/W-rms: 0.22623
[32m[0716 14:19:18 @monitor.py:476][0m param-summary/conv6/W-rms: 0.18883
[32m[0716 14:19:18 @monitor.py:476][0m param-summary/fc1/W-rms: 0.3351
[32m[0716 14:19:18 @monitor.py:476][0m regularize_cost: 2.8717e-05
[32m[0716 14:19:18 @monitor.py:476][0m train_error: 0.020835
[32m[0716 14:19:18 @monitor.py:476][0m val_accuracy: 0.96072
[32m[0716 14:19:18 @monitor.py:476][0m val_cross_entropy_loss: 0.15237
[32m[0716 14:19:18 @group.py:44][0m Callbacks took 9.711 sec in total. InferenceRunner: 9.43 seconds
[32m[0716 14:19:18 @base.py:273][0m Start Epoch 10 ...
[32m[0716 14:23:25 @base.py:283][0m Epoch 10 (global_step 47210) finished, time:4 minutes 6 seconds.
[32m[0716 14:23:25 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-47210.
[32m[0716 14:23:35 @monitor.py:476][0m QueueInput/queue_size: 1.5613e-17
[32m[0716 14:23:35 @monitor.py:476][0m accuracy: 0.98196
[32m[0716 14:23:35 @monitor.py:476][0m cost: 0.072017
[32m[0716 14:23:35 @monitor.py:476][0m cross_entropy_loss: 0.071984
[32m[0716 14:23:35 @monitor.py:476][0m param-summary/conv0/W-rms: 0.3309
[32m[0716 14:23:35 @monitor.py:476][0m param-summary/conv1/W-rms: 0.21778
[32m[0716 14:23:35 @monitor.py:476][0m param-summary/conv2/W-rms: 0.24496
[32m[0716 14:23:35 @monitor.py:476][0m param-summary/conv3/W-rms: 0.2477
[32m[0716 14:23:35 @monitor.py:476][0m param-summary/conv4/W-rms: 0.23059
[32m[0716 14:23:35 @monitor.py:476][0m param-summary/conv5/W-rms: 0.24064
[32m[0716 14:23:35 @monitor.py:476][0m param-summary/conv6/W-rms: 0.19956
[32m[0716 14:23:35 @monitor.py:476][0m param-summary/fc1/W-rms: 0.35869
[32m[0716 14:23:35 @monitor.py:476][0m regularize_cost: 3.292e-05
[32m[0716 14:23:35 @monitor.py:476][0m train_error: 0.018036
[32m[0716 14:23:35 @monitor.py:476][0m val_accuracy: 0.96133
[32m[0716 14:23:35 @monitor.py:476][0m val_cross_entropy_loss: 0.14913
[32m[0716 14:23:35 @group.py:44][0m Callbacks took 9.711 sec in total. InferenceRunner: 9.44 seconds
[32m[0716 14:23:35 @base.py:287][0m Training has finished!
[32m[0716 14:23:35 @input_source.py:177][0m [EnqueueThread] Thread EnqueueThread: enqueue dataflow to TF queue "QueueInput/input_queue" Exited.
