[32m[0715 12:30:18 @logger.py:92][0m Argv: /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-49d3f7f6-7d8c-4bc8-ae1e-7863be7d26fe.json
[32m[0715 12:30:18 @parallel.py:340][0m [MultiProcessRunnerZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0715 12:30:18 @input_source.py:221][0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[32m[0715 12:30:18 @trainers.py:48][0m Building graph for a single training tower ...
[32m[0715 12:30:18 @<ipython-input-7-876926d3e43a>:113][0m Binarizing weight conv1/W
[32m[0715 12:30:18 @<ipython-input-7-876926d3e43a>:113][0m Binarizing weight conv2/W
[32m[0715 12:30:18 @<ipython-input-7-876926d3e43a>:113][0m Binarizing weight conv3/W
[32m[0715 12:30:18 @<ipython-input-7-876926d3e43a>:113][0m Binarizing weight conv4/W
[32m[0715 12:30:18 @<ipython-input-7-876926d3e43a>:113][0m Binarizing weight conv5/W
[32m[0715 12:30:18 @<ipython-input-7-876926d3e43a>:113][0m Binarizing weight conv6/W
[32m[0715 12:30:18 @regularize.py:97][0m regularize_cost() found 1 variables to regularize.
[32m[0715 12:30:18 @regularize.py:21][0m The following tensors will be regularized: fc1/W:0
[32m[0715 12:30:19 @model_utils.py:67][0m [36mList of Trainable Variables: 
[0mname       shape               #elements
---------  ----------------  -----------
conv0/W    [5, 5, 3, 48]            3600
conv0/b    [48]                       48
conv1/W    [3, 3, 48, 64]          27648
bn1/gamma  [64]                       64
bn1/beta   [64]                       64
conv2/W    [3, 3, 64, 64]          36864
bn2/gamma  [64]                       64
bn2/beta   [64]                       64
conv3/W    [3, 3, 64, 128]         73728
bn3/gamma  [128]                     128
bn3/beta   [128]                     128
conv4/W    [3, 3, 128, 128]       147456
bn4/gamma  [128]                     128
bn4/beta   [128]                     128
conv5/W    [3, 3, 128, 128]       147456
bn5/gamma  [128]                     128
bn5/beta   [128]                     128
conv6/W    [5, 5, 128, 512]      1638400
bn6/gamma  [512]                     512
bn6/beta   [512]                     512
fc1/W      [512, 10]                5120
fc1/b      [10]                       10[36m
Number of trainable variables: 22
Number of parameters (elements): 2082378
Storage space needed for all trainable variables: 7.94MB[0m
[32m[0715 12:30:19 @base.py:207][0m Setup callbacks graph ...
[32m[0715 12:30:19 @argtools.py:138][0m [5m[31mWRN[0m Starting a process with 'fork' method is efficient but not safe and may cause deadlock or crash.Use 'forkserver' or 'spawn' method instead if you run into such issues.See https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods on how to set them.
[32m[0715 12:30:19 @argtools.py:138][0m [5m[31mWRN[0m "import prctl" failed! Install python-prctl so that processes can be cleaned with guarantee.
[32m[0715 12:30:19 @summary.py:47][0m [MovingAverageSummary] 3 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[32m[0715 12:30:19 @summary.py:94][0m Summarizing collection 'summaries' of size 20.
[32m[0715 12:30:19 @graph.py:99][0m Applying collection UPDATE_OPS of 12 ops.
[32m[0715 12:30:20 @base.py:228][0m Creating the session ...
[32m[0715 12:30:27 @base.py:234][0m Initializing the session ...
[32m[0715 12:30:27 @base.py:241][0m Graph Finalized.
[32m[0715 12:30:27 @concurrency.py:37][0m Starting EnqueueThread: enqueue dataflow to TF queue "QueueInput/input_queue" ...
[32m[0715 12:30:28 @base.py:273][0m Start Epoch 1 ...
[32m[0715 12:35:52 @base.py:283][0m Epoch 1 (global_step 4721) finished, time:5 minutes 24 seconds.
[32m[0715 12:35:53 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-4721.
[32m[0715 12:35:53 @monitor.py:476][0m QueueInput/queue_size: 20.625
[32m[0715 12:35:53 @monitor.py:476][0m cost: 0.1848
[32m[0715 12:35:53 @monitor.py:476][0m cross_entropy_loss: 0.18479
[32m[0715 12:35:53 @monitor.py:476][0m param-summary/conv0/W-rms: 0.17842
[32m[0715 12:35:53 @monitor.py:476][0m param-summary/conv1/W-rms: 0.076737
[32m[0715 12:35:53 @monitor.py:476][0m param-summary/conv2/W-rms: 0.069951
[32m[0715 12:35:53 @monitor.py:476][0m param-summary/conv3/W-rms: 0.074601
[32m[0715 12:35:53 @monitor.py:476][0m param-summary/conv4/W-rms: 0.062065
[32m[0715 12:35:53 @monitor.py:476][0m param-summary/conv5/W-rms: 0.065856
[32m[0715 12:35:53 @monitor.py:476][0m param-summary/conv6/W-rms: 0.052535
[32m[0715 12:35:53 @monitor.py:476][0m param-summary/fc1/W-rms: 0.1047
[32m[0715 12:35:53 @monitor.py:476][0m regularize_cost: 2.8008e-06
[32m[0715 12:35:53 @base.py:273][0m Start Epoch 2 ...
[32m[0715 12:41:00 @base.py:283][0m Epoch 2 (global_step 9442) finished, time:5 minutes 7 seconds.
[32m[0715 12:41:00 @saver.py:82][0m Model saved to train_log/svhn-dorefa-1,2,4/model-9442.
[32m[0715 12:41:00 @monitor.py:476][0m QueueInput/queue_size: 0.25013
[32m[0715 12:41:00 @monitor.py:476][0m cost: 0.12589
[32m[0715 12:41:00 @monitor.py:476][0m cross_entropy_loss: 0.12589
[32m[0715 12:41:00 @monitor.py:476][0m param-summary/conv0/W-rms: 0.19419
[32m[0715 12:41:00 @monitor.py:476][0m param-summary/conv1/W-rms: 0.091934
[32m[0715 12:41:01 @monitor.py:476][0m param-summary/conv2/W-rms: 0.091719
[32m[0715 12:41:01 @monitor.py:476][0m param-summary/conv3/W-rms: 0.096929
[32m[0715 12:41:01 @monitor.py:476][0m param-summary/conv4/W-rms: 0.086676
[32m[0715 12:41:01 @monitor.py:476][0m param-summary/conv5/W-rms: 0.092124
[32m[0715 12:41:01 @monitor.py:476][0m param-summary/conv6/W-rms: 0.078713
[32m[0715 12:41:01 @monitor.py:476][0m param-summary/fc1/W-rms: 0.13019
[32m[0715 12:41:01 @monitor.py:476][0m regularize_cost: 4.3274e-06
[32m[0715 12:41:01 @base.py:287][0m Training has finished!
[32m[0715 12:41:01 @input_source.py:177][0m [EnqueueThread] Thread EnqueueThread: enqueue dataflow to TF queue "QueueInput/input_queue" Exited.
