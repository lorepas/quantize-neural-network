{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist-dorefa_first&last_avg_pooling.ipynb","provenance":[],"authorship_tag":"ABX9TyNFYBdoL23r3PzfUVUjZnDT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5qzKFEQKqXDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626516970571,"user_tz":-120,"elapsed":21756,"user":{"displayName":"Lorenzo Pasco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgShaitSLZm3zyLVjMQT7ZzTrEV3kQEXha_A9lkVw=s64","userId":"01314717049817932576"}},"outputId":"4e6a4716-c5f5-48b5-bc23-4c910f285b47"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDXKVoHj26H8","executionInfo":{"status":"ok","timestamp":1626516978025,"user_tz":-120,"elapsed":3487,"user":{"displayName":"Lorenzo Pasco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgShaitSLZm3zyLVjMQT7ZzTrEV3kQEXha_A9lkVw=s64","userId":"01314717049817932576"}},"outputId":"46f7d133-cd68-478b-e70c-8d97a417d49d"},"source":["!pip install tensorpack\n","\n","%cd gdrive/MyDrive/SEAI_Project"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tensorpack\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/8c/63e5f5a4a04dea36b75850f9daa885ccbfad64bec1fae0ee4ca9f31b3eaa/tensorpack-0.11-py2.py3-none-any.whl (296kB)\n","\r\u001b[K     |█                               | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 34.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 35.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 40kB 38.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 51kB 28.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 61kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 71kB 29.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 81kB 30.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 92kB 30.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 102kB 32.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 112kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 122kB 32.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 133kB 32.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 143kB 32.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 153kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 163kB 32.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 174kB 32.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 184kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 194kB 32.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 204kB 32.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 215kB 32.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 225kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 235kB 32.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 245kB 32.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 256kB 32.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 266kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 276kB 32.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 286kB 32.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 296kB 32.0MB/s \n","\u001b[?25hRequirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/dist-packages (from tensorpack) (5.4.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorpack) (1.15.0)\n","Collecting msgpack-numpy>=0.4.4.2\n","  Downloading https://files.pythonhosted.org/packages/19/05/05b8d7c69c6abb36a34325cc3150089bdafc359f0a81fb998d93c5d5c737/msgpack_numpy-0.4.7.1-py2.py3-none-any.whl\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorpack) (1.19.5)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from tensorpack) (4.41.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from tensorpack) (1.1.0)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from tensorpack) (0.8.9)\n","Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/dist-packages (from tensorpack) (22.1.0)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack) (1.0.2)\n","Installing collected packages: msgpack-numpy, tensorpack\n","Successfully installed msgpack-numpy-0.4.7.1 tensorpack-0.11\n","/content/gdrive/MyDrive/SEAI_Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"47qPSLMU19HM","executionInfo":{"status":"ok","timestamp":1626518030781,"user_tz":-120,"elapsed":139863,"user":{"displayName":"Lorenzo Pasco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgShaitSLZm3zyLVjMQT7ZzTrEV3kQEXha_A9lkVw=s64","userId":"01314717049817932576"}},"outputId":"42aae246-6577-4817-e7af-58da90fc03cd"},"source":["#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","# File: svhn-digit-dorefa.py\n","# Author: Yuxin Wu\n","\n","import argparse\n","import os\n","import tensorflow as tf\n","\n","from tensorpack import *\n","from tensorpack.dataflow import dataset\n","from tensorpack.tfutils.summary import add_moving_summary, add_param_summary\n","from tensorpack.tfutils.varreplace import remap_variables\n","\n","\"\"\"\n","This is a tensorpack script for the SVHN results in paper:\n","DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\n","http://arxiv.org/abs/1606.06160\n","The original experiements are performed on a proprietary framework.\n","This is our attempt to reproduce it on tensorpack.\n","Accuracy:\n","    With (W,A,G)=(1,1,4), can reach 3.1~3.2% error after 150 epochs.\n","    With (W,A,G)=(1,2,4), error is 3.0~3.1%.\n","    With (W,A,G)=(32,32,32), error is about 2.3%.\n","Speed:\n","    With quantization, 60 batch/s on 1 1080Ti. (4721 batch / epoch)\n","To Run:\n","    ./svhn-digit-dorefa.py --dorefa 1,2,4\n","\"\"\"\n","tf.compat.v1.reset_default_graph()\n","\n","BITW = 1\n","BITA = 2\n","BITG = 4\n","\n","\"\"\"\n","imported from dorefa file\n","\"\"\"\n","def get_dorefa(bitW, bitA, bitG):\n","    \"\"\"\n","    Return the three quantization functions fw, fa, fg, for weights, activations and gradients respectively\n","    \"\"\"\n","    def quantize(x, k):\n","        n = float(2 ** k - 1)\n","\n","        @tf.custom_gradient\n","        def _quantize(x):\n","            return tf.round(x * n) / n, lambda dy: dy\n","\n","        return _quantize(x)\n","\n","    def fw(x):\n","        if bitW == 32:\n","            return x\n","\n","        if bitW == 1:   # BWN\n","            E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))\n","\n","            @tf.custom_gradient\n","            def _sign(x):\n","                return tf.where(tf.equal(x, 0), tf.ones_like(x), tf.sign(x / E)) * E, lambda dy: dy\n","\n","            return _sign(x)\n","\n","        x = tf.tanh(x)\n","        x = x / tf.reduce_max(tf.abs(x)) * 0.5 + 0.5\n","        return 2 * quantize(x, bitW) - 1\n","\n","    def fa(x):\n","        if bitA == 32:\n","            return x\n","        return quantize(x, bitA)\n","\n","    def fg(x):\n","        if bitG == 32:\n","            return x\n","\n","        @tf.custom_gradient\n","        def _identity(input):\n","            def grad_fg(x):\n","                rank = x.get_shape().ndims\n","                assert rank is not None\n","                maxx = tf.reduce_max(tf.abs(x), list(range(1, rank)), keepdims=True)\n","                x = x / maxx\n","                n = float(2**bitG - 1)\n","                x = x * 0.5 + 0.5 + tf.random.uniform(\n","                    tf.shape(x), minval=-0.5 / n, maxval=0.5 / n)\n","                x = tf.clip_by_value(x, 0.0, 1.0)\n","                x = quantize(x, bitG) - 0.5\n","                return x * maxx * 2\n","\n","            return input, grad_fg\n","\n","        return _identity(x)\n","    return fw, fa, fg\n","\n","\n","class Model(ModelDesc):\n","    def inputs(self):\n","        return [tf.TensorSpec([None, 40, 40], tf.float32, 'input'),\n","                tf.TensorSpec([None], tf.int32, 'label')]\n","\n","    def build_graph(self, image, label):\n","        fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n","\n","        # monkey-patch tf.get_variable to apply fw\n","        def binarize_weight(v):\n","            name = v.op.name\n","            # don't binarize first and last layer\n","            if not name.endswith('W'):\n","                return v\n","            else:\n","                logger.info(\"Binarizing weight {}\".format(v.op.name))\n","                return fw(v)\n","\n","        def nonlin(x):\n","            if BITA == 32:\n","                return tf.nn.relu(x)\n","            return tf.clip_by_value(x, 0.0, 1.0)\n","\n","        def activate(x):\n","            return fa(nonlin(x))\n","\n","        image = tf.expand_dims(image, 3)\n","        image = image / 256.0\n","\n","        with remap_variables(binarize_weight), \\\n","                argscope(BatchNorm, momentum=0.9, epsilon=1e-4), \\\n","                argscope(Conv2D, use_bias=False):\n","            logits = (LinearWrap(image)\n","                      .Conv2D('conv0', 48, 5, padding='VALID', use_bias=True)\n","                      .AvgPooling('pool0', 2, padding='SAME')\n","                      .apply(activate)\n","                      # 18\n","                      .Conv2D('conv1', 64, 3, padding='SAME')\n","                      .apply(fg)\n","                      .BatchNorm('bn1').apply(activate)\n","#AVGPooling\n","                      .Conv2D('conv2', 64, 3, padding='SAME')\n","                      .apply(fg)\n","                      .BatchNorm('bn2')\n","                      .AvgPooling('pool1', 2, padding='SAME')\n","                      .apply(activate)\n","                      # 9\n","                      .Conv2D('conv3', 128, 3, padding='VALID')\n","                      .apply(fg)\n","                      .BatchNorm('bn3').apply(activate)\n","                      # 7\n","\n","                      .Conv2D('conv4', 128, 3, padding='SAME')\n","                      .apply(fg)\n","                      .BatchNorm('bn4').apply(activate)\n","\n","                      .Conv2D('conv5', 128, 3, padding='VALID')\n","                      .apply(fg)\n","                      .BatchNorm('bn5').apply(activate)\n","                      # 5\n","                      .Dropout(rate=0.5 if self.training else 0.0)\n","                      .Conv2D('conv6', 512, 5, padding='VALID')\n","                      .apply(fg).BatchNorm('bn6')\n","                      .apply(nonlin)\n","                      .FullyConnected('fc1', 10)())\n","        tf.nn.softmax(logits, name='output')\n","\n","        correct = tf.cast(tf.nn.in_top_k(predictions=logits, targets=label, k=1), tf.float32, name='correct')\n","        accuracy = tf.reduce_mean(correct, name='accuracy')\n","        train_error = tf.reduce_mean(1 - correct, name='train_error')\n","        summary.add_moving_summary(train_error, accuracy)\n","        \n","        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n","        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n","        # weight decay on all W of fc layers\n","        wd_cost = regularize_cost('fc.*/W', l2_regularizer(1e-7))\n","        add_param_summary(('.*/W', ['histogram', 'rms']))\n","        total_cost = tf.add_n([cost, wd_cost], name='cost')\n","        add_moving_summary(cost, wd_cost, total_cost)\n","        return total_cost\n","\n","    def optimizer(self):\n","        lr = tf.compat.v1.train.exponential_decay(\n","            learning_rate=1e-3,\n","            global_step=get_global_step_var(),\n","            decay_steps=4721 * 100,\n","            decay_rate=0.5, staircase=True, name='learning_rate')\n","        tf.summary.scalar('lr', lr)\n","\n","        return tf.compat.v1.train.AdamOptimizer(lr, epsilon=1e-5)\n","\n","\n","def get_config():\n","    logger.set_logger_dir(os.path.join('train_log', 'mnist-dorefa-{}'.format(args)))\n","\n","    # prepare dataset\n","    data_train = dataset.Mnist('train', shuffle=True)\n","    data_test = dataset.Mnist('test', shuffle=True)\n","\n","    augmentors = [imgaug.Resize((40, 40))]\n","    data_train = AugmentImageComponent(data_train, augmentors)\n","    data_train = BatchData(data_train, 128)\n","    data_train = MultiProcessRunnerZMQ(data_train, 5)\n","\n","    augmentors = [imgaug.Resize((40, 40))]\n","    data_test = AugmentImageComponent(data_test, augmentors)\n","    data_test = BatchData(data_test, 128, remainder=True)\n","\n","    return TrainConfig(\n","        data=QueueInput(data_train),\n","        callbacks=[\n","            ModelSaver(),\n","            InferenceRunner(    # run inference(for validation) after every epoch\n","                data_test,   # the DataFlow instance used for validation\n","                ScalarStats(    # produce `val_accuracy` and `val_cross_entropy_loss`\n","                    ['cross_entropy_loss', 'accuracy'], prefix='val'))\n","        ],\n","        model=Model(),\n","        max_epoch=10,\n","    )\n","\n","args = \"1,2,4\"\n","BITW, BITA, BITG = map(int, args.split(','))\n","config = get_config()\n","launch_train_with_config(config, SimpleTrainer())\n","\n","'''\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--dorefa',\n","                        help='number of bits for W,A,G, separated by comma. Defaults to \\'1,2,4\\'',\n","                        default='1,2,4')\n","    args = parser.parse_args()\n","\n","    BITW, BITA, BITG = map(int, args.dorefa.split(','))\n","    config = get_config()\n","    launch_train_with_config(config, SimpleTrainer())\n","'''"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\u001b[32m[0717 10:31:31 @logger.py:128]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Log directory train_log/mnist-dorefa-1,2,4 exists! Use 'd' to delete it. \n","\u001b[32m[0717 10:31:31 @logger.py:131]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m If you're resuming from a previous run, you can choose to keep it.\n","Press any other key to exit. \n","Select Action: k (keep) / d (delete) / q (quit):k\n","\u001b[32m[0717 10:31:34 @logger.py:85]\u001b[0m Existing log file 'train_log/mnist-dorefa-1,2,4/log.log' backuped to 'train_log/mnist-dorefa-1,2,4/log.log.0717-103134'\n","\u001b[32m[0717 10:31:34 @logger.py:92]\u001b[0m Argv: /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-728d80ac-4555-4ad0-a400-d2e2f90775c2.json\n","\u001b[32m[0717 10:31:34 @parallel.py:340]\u001b[0m [MultiProcessRunnerZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n","\u001b[32m[0717 10:31:34 @input_source.py:221]\u001b[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n","\u001b[32m[0717 10:31:34 @trainers.py:48]\u001b[0m Building graph for a single training tower ...\n","\u001b[32m[0717 10:31:34 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv0/W\n","\u001b[32m[0717 10:31:34 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv1/W\n","\u001b[32m[0717 10:31:34 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv2/W\n","\u001b[32m[0717 10:31:34 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv3/W\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:31:34 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv4/W\n","\u001b[32m[0717 10:31:34 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv5/W\n","\u001b[32m[0717 10:31:34 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv6/W\n","\u001b[32m[0717 10:31:35 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight fc1/W\n","\u001b[32m[0717 10:31:35 @regularize.py:97]\u001b[0m regularize_cost() found 1 variables to regularize.\n","\u001b[32m[0717 10:31:35 @regularize.py:21]\u001b[0m The following tensors will be regularized: fc1/W:0\n","\u001b[32m[0717 10:31:35 @model_utils.py:67]\u001b[0m \u001b[36mList of Trainable Variables: \n","\u001b[0mname       shape               #elements\n","---------  ----------------  -----------\n","conv0/W    [5, 5, 1, 48]            1200\n","conv0/b    [48]                       48\n","conv1/W    [3, 3, 48, 64]          27648\n","bn1/gamma  [64]                       64\n","bn1/beta   [64]                       64\n","conv2/W    [3, 3, 64, 64]          36864\n","bn2/gamma  [64]                       64\n","bn2/beta   [64]                       64\n","conv3/W    [3, 3, 64, 128]         73728\n","bn3/gamma  [128]                     128\n","bn3/beta   [128]                     128\n","conv4/W    [3, 3, 128, 128]       147456\n","bn4/gamma  [128]                     128\n","bn4/beta   [128]                     128\n","conv5/W    [3, 3, 128, 128]       147456\n","bn5/gamma  [128]                     128\n","bn5/beta   [128]                     128\n","conv6/W    [5, 5, 128, 512]      1638400\n","bn6/gamma  [512]                     512\n","bn6/beta   [512]                     512\n","fc1/W      [512, 10]                5120\n","fc1/b      [10]                       10\u001b[36m\n","Number of trainable variables: 22\n","Number of parameters (elements): 2079978\n","Storage space needed for all trainable variables: 7.93MB\u001b[0m\n","\u001b[32m[0717 10:31:35 @base.py:207]\u001b[0m Setup callbacks graph ...\n","\u001b[32m[0717 10:31:35 @argtools.py:138]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m \"import prctl\" failed! Install python-prctl so that processes can be cleaned with guarantee.\n","\u001b[32m[0717 10:31:36 @inference_runner.py:148]\u001b[0m [InferenceRunner] Building tower 'InferenceTower' on device /gpu:0 ...\n","\u001b[32m[0717 10:31:36 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv0/W\n","\u001b[32m[0717 10:31:37 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv1/W\n","\u001b[32m[0717 10:31:37 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv2/W\n","\u001b[32m[0717 10:31:38 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv3/W\n","\u001b[32m[0717 10:31:38 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv4/W\n","\u001b[32m[0717 10:31:38 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv5/W\n","\u001b[32m[0717 10:31:38 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight conv6/W\n","\u001b[32m[0717 10:31:38 @<ipython-input-4-7ff13444fb02>:113]\u001b[0m Binarizing weight fc1/W\n","\u001b[32m[0717 10:31:38 @summary.py:47]\u001b[0m [MovingAverageSummary] 5 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.\n","\u001b[32m[0717 10:31:38 @summary.py:94]\u001b[0m Summarizing collection 'summaries' of size 22.\n","\u001b[32m[0717 10:31:38 @graph.py:99]\u001b[0m Applying collection UPDATE_OPS of 12 ops.\n","\u001b[32m[0717 10:31:38 @base.py:228]\u001b[0m Creating the session ...\n","\u001b[32m[0717 10:31:39 @base.py:234]\u001b[0m Initializing the session ...\n","\u001b[32m[0717 10:31:39 @base.py:241]\u001b[0m Graph Finalized.\n","\u001b[32m[0717 10:31:39 @concurrency.py:37]\u001b[0m Starting EnqueueThread: enqueue dataflow to TF queue \"QueueInput/input_queue\" ...\n","\u001b[32m[0717 10:31:39 @inference_runner.py:95]\u001b[0m [InferenceRunner] Will eval 79 iterations\n","\u001b[32m[0717 10:31:39 @monitor.py:361]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m History epoch=29 from JSON is not the predecessor of the current starting_epoch=1\n","\u001b[32m[0717 10:31:39 @monitor.py:362]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m If you want to resume old training, either use `AutoResumeTrainConfig` or correctly set the new starting_epoch yourself to avoid inconsistency. \n","\u001b[32m[0717 10:31:39 @monitor.py:369]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Now, we will train with starting_epoch=1 and backup old json to train_log/mnist-dorefa-1,2,4/stats.json.0717-103139\n","\u001b[32m[0717 10:31:39 @base.py:273]\u001b[0m Start Epoch 1 ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|##########|468/468[00:13<00:00,35.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:31:52 @base.py:283]\u001b[0m Epoch 1 (global_step 468) finished, time:13.1 seconds.\n","\u001b[32m[0717 10:31:52 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-468.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,84.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m accuracy: 0.12174\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m cost: 2.3008\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3008\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.29315\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.063531\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m regularize_cost: 1.0331e-06\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m train_error: 0.87826\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m val_accuracy: 0.11294\n","\u001b[32m[0717 10:31:53 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.301\n","\u001b[32m[0717 10:31:53 @base.py:273]\u001b[0m Start Epoch 2 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:11<00:00,39.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:05 @base.py:283]\u001b[0m Epoch 2 (global_step 936) finished, time:12 seconds.\n","\u001b[32m[0717 10:32:05 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-936.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,89.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m accuracy: 0.11469\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m cost: 2.3005\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3005\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.29748\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.063568\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m regularize_cost: 1.0327e-06\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m train_error: 0.88531\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m val_accuracy: 0.11432\n","\u001b[32m[0717 10:32:06 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.301\n","\u001b[32m[0717 10:32:06 @base.py:273]\u001b[0m Start Epoch 3 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:11<00:00,39.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:18 @base.py:283]\u001b[0m Epoch 3 (global_step 1404) finished, time:11.9 seconds.\n","\u001b[32m[0717 10:32:18 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-1404.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,91.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m accuracy: 0.1182\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m cost: 2.3002\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3001\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.3018\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.063623\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m regularize_cost: 1.035e-06\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m train_error: 0.8818\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m val_accuracy: 0.11224\n","\u001b[32m[0717 10:32:19 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.3011\n","\u001b[32m[0717 10:32:19 @base.py:273]\u001b[0m Start Epoch 4 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:11<00:00,39.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:31 @base.py:283]\u001b[0m Epoch 4 (global_step 1872) finished, time:11.9 seconds.\n","\u001b[32m[0717 10:32:31 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-1872.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,92.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m accuracy: 0.11786\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m cost: 2.3003\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3003\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.30408\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.063383\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m regularize_cost: 1.0273e-06\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m train_error: 0.88214\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m val_accuracy: 0.11363\n","\u001b[32m[0717 10:32:32 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.3012\n","\u001b[32m[0717 10:32:32 @base.py:273]\u001b[0m Start Epoch 5 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:12<00:00,38.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:44 @base.py:283]\u001b[0m Epoch 5 (global_step 2340) finished, time:12.2 seconds.\n","\u001b[32m[0717 10:32:44 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-2340.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,88.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m accuracy: 0.109\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m cost: 2.3019\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3019\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.30553\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.062916\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m regularize_cost: 1.0136e-06\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m train_error: 0.891\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m val_accuracy: 0.11294\n","\u001b[32m[0717 10:32:45 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.301\n","\u001b[32m[0717 10:32:45 @base.py:273]\u001b[0m Start Epoch 6 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:11<00:00,39.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:57 @base.py:283]\u001b[0m Epoch 6 (global_step 2808) finished, time:11.9 seconds.\n","\u001b[32m[0717 10:32:57 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-2808.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,93.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m accuracy: 0.11961\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m cost: 2.3\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.3084\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.062802\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m regularize_cost: 1.0096e-06\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m train_error: 0.88039\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m val_accuracy: 0.11224\n","\u001b[32m[0717 10:32:58 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.3011\n","\u001b[32m[0717 10:32:58 @base.py:273]\u001b[0m Start Epoch 7 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:11<00:00,39.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:33:10 @base.py:283]\u001b[0m Epoch 7 (global_step 3276) finished, time:12 seconds.\n","\u001b[32m[0717 10:33:10 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-3276.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,87.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m accuracy: 0.10927\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m cost: 2.3019\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3019\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.31103\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.062431\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m regularize_cost: 9.9824e-07\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m train_error: 0.89073\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m val_accuracy: 0.11363\n","\u001b[32m[0717 10:33:11 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.3014\n","\u001b[32m[0717 10:33:11 @base.py:273]\u001b[0m Start Epoch 8 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:12<00:00,38.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:33:24 @base.py:283]\u001b[0m Epoch 8 (global_step 3744) finished, time:12.2 seconds.\n","\u001b[32m[0717 10:33:24 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-3744.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,88.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m accuracy: 0.11265\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m cost: 2.3017\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3017\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.31159\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.062294\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m regularize_cost: 9.9374e-07\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m train_error: 0.88735\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m val_accuracy: 0.11294\n","\u001b[32m[0717 10:33:25 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.3011\n","\u001b[32m[0717 10:33:25 @base.py:273]\u001b[0m Start Epoch 9 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:12<00:00,38.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:33:37 @base.py:283]\u001b[0m Epoch 9 (global_step 4212) finished, time:12 seconds.\n","\u001b[32m[0717 10:33:37 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-4212.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,92.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m accuracy: 0.11004\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m cost: 2.301\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.301\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.3123\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.062123\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m regularize_cost: 9.881e-07\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m train_error: 0.88996\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m val_accuracy: 0.11294\n","\u001b[32m[0717 10:33:38 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.3013\n","\u001b[32m[0717 10:33:38 @base.py:273]\u001b[0m Start Epoch 10 ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|468/468[00:11<00:00,40.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:33:49 @base.py:283]\u001b[0m Epoch 10 (global_step 4680) finished, time:11.7 seconds.\n","\u001b[32m[0717 10:33:49 @saver.py:82]\u001b[0m Model saved to train_log/mnist-dorefa-1,2,4/model-4680.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|##########|79/79[00:00<00:00,90.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m QueueInput/queue_size: 50\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m accuracy: 0.1124\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m cost: 2.3018\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m cross_entropy_loss: 2.3018\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m param-summary/conv0/W-rms: 0.31204\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m param-summary/conv1/W-rms: 0.068168\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m param-summary/conv2/W-rms: 0.05887\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m param-summary/conv3/W-rms: 0.058843\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m param-summary/conv4/W-rms: 0.041688\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m param-summary/conv5/W-rms: 0.04158\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m param-summary/conv6/W-rms: 0.024997\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m param-summary/fc1/W-rms: 0.061772\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m regularize_cost: 9.7736e-07\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m train_error: 0.8876\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m val_accuracy: 0.11363\n","\u001b[32m[0717 10:33:50 @monitor.py:476]\u001b[0m val_cross_entropy_loss: 2.3009\n","\u001b[32m[0717 10:33:50 @base.py:287]\u001b[0m Training has finished!\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nif __name__ == '__main__':\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument('--dorefa',\\n                        help='number of bits for W,A,G, separated by comma. Defaults to '1,2,4'',\\n                        default='1,2,4')\\n    args = parser.parse_args()\\n\\n    BITW, BITA, BITG = map(int, args.dorefa.split(','))\\n    config = get_config()\\n    launch_train_with_config(config, SimpleTrainer())\\n\""]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"mduuAqCeuc4B"},"source":["import json\n","import matplotlib.pyplot as plt\n","\n","f = open(\"train_log/mnist-dorefa-1,2,4/stats_def_first&last_avg_pooling.json\",\"r\")\n","\n","data = json.load(f)\n","accuracy = []\n","val_accuracy = []\n","for ob in data:\n","  accuracy.append(ob[\"accuracy\"])\n","  val_accuracy.append(ob[\"val_accuracy\"])\n","\n","epochs = range(len(accuracy))\n","\n","plt.plot(epochs, accuracy, 'r', label='Training acc')\n","plt.plot(epochs, val_accuracy, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ST3WBdVha1W-","executionInfo":{"status":"ok","timestamp":1626514638069,"user_tz":-120,"elapsed":223,"user":{"displayName":"Lorenzo Pasco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgShaitSLZm3zyLVjMQT7ZzTrEV3kQEXha_A9lkVw=s64","userId":"01314717049817932576"}},"outputId":"96297c51-9ecd-428a-acaa-186f2d72160e"},"source":["from tabulate import tabulate\n","import matplotlib.pyplot as plt\n","\n","ep = [i+1 for i in epochs]\n","table_acc = {\"Epochs\" : ep, \"Accuracy\":accuracy}\n","table_val_acc = {\"Epochs\" : ep, \"Accuracy\":val_accuracy}\n","\n","print(\"ACCURACY\\n\")\n","print(tabulate(table_acc, headers='keys', tablefmt='fancy_grid'))\n","print(\"\\nVALIDATION ACCURACY\\n\")\n","print(tabulate(table_val_acc, headers='keys', tablefmt='fancy_grid'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ACCURACY\n","\n","╒══════════╤════════════╕\n","│   Epochs │   Accuracy │\n","╞══════════╪════════════╡\n","│        1 │   0.967162 │\n","├──────────┼────────────┤\n","│        2 │   0.974561 │\n","├──────────┼────────────┤\n","│        3 │   0.982541 │\n","├──────────┼────────────┤\n","│        4 │   0.982901 │\n","├──────────┼────────────┤\n","│        5 │   0.985293 │\n","├──────────┼────────────┤\n","│        6 │   0.987453 │\n","├──────────┼────────────┤\n","│        7 │   0.99122  │\n","├──────────┼────────────┤\n","│        8 │   0.990215 │\n","├──────────┼────────────┤\n","│        9 │   0.992805 │\n","├──────────┼────────────┤\n","│       10 │   0.993367 │\n","╘══════════╧════════════╛\n","\n","VALIDATION ACCURACY\n","\n","╒══════════╤════════════╕\n","│   Epochs │   Accuracy │\n","╞══════════╪════════════╡\n","│        1 │   0.976464 │\n","├──────────┼────────────┤\n","│        2 │   0.979727 │\n","├──────────┼────────────┤\n","│        3 │   0.983782 │\n","├──────────┼────────────┤\n","│        4 │   0.987441 │\n","├──────────┼────────────┤\n","│        5 │   0.984177 │\n","├──────────┼────────────┤\n","│        6 │   0.973596 │\n","├──────────┼────────────┤\n","│        7 │   0.991396 │\n","├──────────┼────────────┤\n","│        8 │   0.988924 │\n","├──────────┼────────────┤\n","│        9 │   0.988825 │\n","├──────────┼────────────┤\n","│       10 │   0.9911   │\n","╘══════════╧════════════╛\n"],"name":"stdout"}]}]}